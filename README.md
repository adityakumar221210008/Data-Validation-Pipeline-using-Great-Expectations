ğŸ“Š Data Validation Pipeline using Great Expectations

ğŸ“Œ Overview

This project demonstrates a complete data validation pipeline built using Great Expectations and Python.
It validates structured tabular data by enforcing data quality rules such as schema checks, null validation, range validation, and consistency checks.

The project simulates a real-world data quality workflow used in analytics, data engineering, and ETL pipelines.

ğŸš€ Features

Automated data quality checks

Column-level validations (type, nulls, ranges)

Schema enforcement

Data profiling and documentation

HTML-based Data Docs generation

Easily extendable to production pipelines

ğŸ› ï¸ Technologies Used

Python 3.10

Great Expectations 0.17.x

Pandas

Jupyter Notebook

ğŸ“‚ Project Structure
data-validation-project/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ amazon.csv
â”‚
â”œâ”€â”€ great_expectations/
â”‚   â”œâ”€â”€ expectations/
â”‚   â”œâ”€â”€ validations/
â”‚   â””â”€â”€ data_docs/
â”‚
â”œâ”€â”€ data_validation_pipeline.ipynb
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ“‹ Validation Rules Implemented

âœ” Column existence checks
âœ” Non-null value checks
âœ” Value range validation (e.g., ratings between 0â€“5)
âœ” Data type validation
âœ” Automated reporting via Data Docs

â–¶ï¸ How to Run the Project
1ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

2ï¸âƒ£ Run the Notebook
jupyter notebook


Open data_validation_pipeline.ipynb and run all cells.

ğŸ“Š Output

Validation results printed in the notebook

HTML data quality report generated in:

great_expectations/data_docs/


Clear pass/fail summary for each validation rule

ğŸ§ª Example Validations

product_id must not be null

rating must be between 0 and 5

discounted_price must be a positive number

ğŸ“ˆ Use Cases

Data quality validation before analytics

ETL pipeline validation

Data engineering projects

Machine learning preprocessing checks

ğŸ§  Learnings

This project demonstrates:

How to build reproducible data quality pipelines

How to use Great Expectations effectively

Best practices for validating structured datasets
